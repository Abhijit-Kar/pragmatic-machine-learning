{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "\n",
    "Combines predictions of several estimators\n",
    "\n",
    "## Methods\n",
    "\n",
    "### Averaging Method\n",
    "1. Several estimators are built independently and then their predictions are averaged\n",
    "1. Better because variance is reduced\n",
    "1. Works best with strong & complex models\n",
    "1. e.g. [Bagging Methods](http://scikit-learn.org/stable/modules/ensemble.html#bagging-meta-estimator), Random Forest\n",
    "\n",
    "### Boosting Method\n",
    "1. Base estimators are built sequentially and one tries to reduce the bias of the combined estimator\n",
    "1. Works best with weak models\n",
    "1. e.g. [AdaBoost](http://scikit-learn.org/stable/modules/ensemble.html#adaboost), Gradient Tree Boosting\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/ensemble.html#ensemble-methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "1. Widely Used\n",
    "1. A random forest is essentially a collection of decision trees, where each tree is slightly different from the others\n",
    "1. Unlikely to overfit\n",
    "1. For regression output is based on average of all random trees\n",
    "1. For classification output is based on soft voting\n",
    "1. Gives out feature importance\n",
    "1. Can be parallelized\n",
    "\n",
    "**Disadvantage**\n",
    "1. Random forests donâ€™t tend to perform well on very high dimensional, sparse data, such as text\n",
    "1. Interpreting randomly generated trees is difficult, hence in that case Decision Tree can be used\n",
    "1. random forests require more memory and are slower to train and to predict than linear models\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = pd.read_csv('../data/iris.csv', dtype = {'species': 'category'})\n",
    "iris_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris_df.iloc[:, :-1]\n",
    "y = iris_df.species\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94730392156862742"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth = 2)\n",
    "cross_val_score(clf, X, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9673202614379085"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 10, max_depth = None, min_samples_split = 2, max_features = 4)\n",
    "cross_val_score(clf, X, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Tree Boosting\n",
    "1. Used in Web Search Ranking\n",
    "1. Widely Used\n",
    "1. Unline Random Forest Gradient boosting works by building trees in a serial manner, where each tree tries to correct the mistakes of the previous one\n",
    "1. There's no randomization by default\n",
    "1. Strong pre-pruning is used\n",
    "1. Use very shallow trees, of depth one to five, which makes the model smaller in terms of memory and makes predictions faster\n",
    "1. Param (learning_rate): Higher implies complex models\n",
    "1. Outputs feature importance as well\n",
    "\n",
    "**Advantages**\n",
    "1. Can handle heterogeneous features\n",
    "1. Predictive power\n",
    "1. Robustness to outliers in output space\n",
    "1. Winning entries in competetions\n",
    "\n",
    "**Disadvantages**\n",
    "1. Due to the sequential nature of boosting it can hardly be parallelized\n",
    "1. It can't be scaled\n",
    "1. Sensitive to parameters\n",
    "1. Requires careful tuning of the parameters and may take a long time to train\n",
    "1. As with other tree-based models, it also often does not work well on high-dimensional sparse data.\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=1.0, loss='deviance', max_depth=2,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators = 100, learning_rate = 1.0, max_depth = 2)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier\n",
    "1. Combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities\n",
    "1. Used with equally well performing models to balance out their weaknesses\n",
    "\n",
    "> http://scikit-learn.org/stable/modules/ensemble.html#voting-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96 (+/- 0.04) [Logistic Regression]\n",
      "Accuracy: 0.95 (+/- 0.03) [Random Forest]\n",
      "Accuracy: 0.95 (+/- 0.03) [Naive Bayes]\n",
      "Accuracy: 0.96 (+/- 0.02) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "vclf = VotingClassifier(estimators = [('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, vclf], ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv = 5, scoring = 'accuracy')\n",
    "    print(\"Accuracy: {:.2f} (+/- {:.2f}) [{}]\".format(scores.mean(), scores.std(), label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
